---
title: "Lecture 3"
author: "Kodie McNamara"
date: "September 27, 2021"
output: html_document
---

### Forecasting Strategies 
First, we read in the Build.dat dataset. As always, make sure your directory is set.
```{r}
# Set your working directory below if it is not already set to the current 'Lecture Notes' directory.
# Your directory must be set to tell R where to find the data file for Maine monthly unemployment

#setwd("C:/Users/kmac4/Documents/St. Elizabeth University/CS-637-Time-Series-and-Forecasting-/Lecture Notes")
Build.dat = read.csv(file = "data/ApprovActiv.csv")
```

The Australian Bureau of Statistics publishes detailed data on building approvals for each month, and, a few weeks later, the Building Activity Publication lists the value of building work done in each quarter. The data in the file ApprovActiv.csv are the total dwellings approved per month, averaged over the past three months, labelled “Approvals”, and the value of work done over the past three months (chain volume measured in millions of Australian dollars at the reference year 2004–05 prices), labelled “Activity”, from March 1996 until September 2006. We start by reading the data into R and then construct time series objects and plot the two series on the same graph using `ts.plot` 

```{r}
App.ts <- ts(Build.dat$Approvals, start = c(1996,1), freq=4)
Act.ts <- ts(Build.dat$Activity, start = c(1996,1), freq=4)
ts.plot(App.ts, Act.ts, lty = c(1,3))
```

#### Auto-Correlation, Cross Covariance and Cross Correlation
The ts.union function binds time series with a common frequency, padding with ‘NA’s to the union of their time coverages. If ts.union is used within the acf command, R returns the correlograms for the two variables and the cross-correlograms in a single figure
```{r}
acf(ts.union(App.ts, Act.ts))
```

Here we remove the trend using decompose, which uses a centred moving average of the four quarters
```{r}
app.ran <- decompose(App.ts)$random
app.ran.ts <- window (app.ran, start = c(1996, 3), end = c(2006, 1))
act.ran <- decompose (Act.ts)$random
act.ran.ts <- window (act.ran, start = c(1996, 3), end = c(2006, 1))
acf (ts.union(app.ran.ts, act.ran.ts))
ccf (app.ran.ts, act.ran.ts)
```
We again use print() to obtain the following table.
```{r}
print(acf(ts.union(app.ran.ts, act.ran.ts)))
```


#### Bass Model
We show a typical Bass curve by fitting sales per unit time to yearly sales of VCRs in the US home market between 1980 and 1989 (Bass website) using the R non-linear least squares function `nls`. The variable `T79` is the year from 1979, and the variable `Tdelt` is the time from 1979 at a finer resolution of 0.1 year for plotting the Bass curves. The cumulative sum function `cumsum` is useful for monitoring changes in the mean level of the process

```{r}
T79 <- 1:10
Tdelt <- (1:100) / 10
Sales <- c(840, 1470, 2110, 4000, 7590, 10950, 10530, 9470, 7790, 5890)
Cusales <- cumsum(Sales)
Bass.nls <- nls(Sales ~ M * ( ((P+Q)^2 / P) * exp(-(P+Q) * T79) ) /
                  (1+(Q/P)*exp(-(P+Q)*T79))^2, 
                start = list(M=60630, P=0.03, Q=0.38))
summary(Bass.nls)
```

The final estimates for `m`, `p`, and `q`, rounded to two significant places, are 68000, 0.0066, and 0.64 respectively. The starting values for P and Q are `p` and `q` for a typical product. We assume the sales figures are prone to error and estimate the total sales, `m`, setting the starting value for M to the recorded total sales. The data and fitted curve can be plotted using the code below

```{r}
Bcoef <- coef(Bass.nls)
m <- Bcoef[1]
p <- Bcoef[2]
q <- Bcoef[3]
ngete <- exp(-(p+q) * Tdelt)
Bpdf <- m * ( (p+q)^2 / p ) * ngete / (1 + (q/p) * ngete)^2
plot(Tdelt, Bpdf, xlab = "Year from 1979",
     ylab = "Sales per year", type='l')
points(T79, Sales)
Bcdf <- m * (1 - ngete)/(1 + (q/p)*ngete)
plot(Tdelt, Bcdf, xlab = "Year from 1979",
     ylab = "Cumulative sales", type='l')
points(T79, Cusales)
```

#### Exponential Smoothing
The number of letters of complaint received each month by a motoring organisation over the four years 1996 to 1999 are available on the website. At the beginning of the year 2000, the organisation wishes to estimate the current level of complaints and investigate any trend in the level of complaints. We should first plot the data, and, even though there are only four years of data, we should check for any marked systematic trend or seasonal effects.
```{r}
Motor.dat <- read.csv("data/motororg.csv")
Comp.ts <- ts(Motor.dat$complaints, start = c(1996, 1), freq = 12)
plot(Comp.ts, xlab = "Time / months", ylab = "Complaints")
```


Exponential smoothing is a special case of the Holt-Winters algorithm, which we introduce in the next section, and is implemented in R using the `HoltWinters` function with the additional parameters set to 0. If we do not specify a value for $\alpha$, R will find the value that minimises the one-step-ahead prediction error.
```{r}
Comp.hw1 <- HoltWinters(Comp.ts, beta = 0, gamma = 0)
Comp.hw1
plot(Comp.hw1)
```

#### Holt-Winters Method
The data in the file wine.dat are monthly sales of Australian wine by category, in thousands of litres, from January 1980 until July 1995. The categories are fortified white, dry white, sweet white, red, rose, and sparkling.


```{r}
wine.dat <- read.csv("data/wine.csv")
sweetw.ts <- ts(wine.dat$sweetw, start = c(1980,1), freq = 12)
plot(sweetw.ts, xlab= "Time (months)", ylab = "sales (1000 litres)")
```

The seasonal variation looks as though it would be better modeled as multiplicative, and comparison of the SS1PE for the fitted models confirms this. Here we present results for the model with multiplicative seasonals only
```{r}
sweetw.hw <- HoltWinters (sweetw.ts, seasonal = "mult")
sweetw.hw
sweetw.hw$coef
sweetw.hw$SSE
```

Sales of Australian white wine: fitted values; level; slope (labelled trend); seasonal variation
Sales of Australian white wine and Holt-Winters fitted values.

```{r}
plot (sweetw.hw$fitted)
plot (sweetw.hw)
```


































